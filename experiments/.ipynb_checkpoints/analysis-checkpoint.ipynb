{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nickj/miniconda3/envs/vl/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import json\n",
    "import random\n",
    "import os\n",
    "import pickle\n",
    "from experiments.embeddings import coco_img_id_to_path\n",
    "\n",
    "evaluator = pickle.load(open('/home/nickj/vl-hallucination/experiments/chair.pkl', \"rb\"))\n",
    "\n",
    "# with open(\"/home/nickj/vl-hallucination/data/coco/dataset_coco.json\", \"r\") as f:\n",
    "#   coco_data = json.load(f)\n",
    "\n",
    "# train_objs = []\n",
    "# for coco_obj in coco_data[\"images\"]:\n",
    "#   if coco_obj[\"split\"] == \"train\":\n",
    "#     train_objs.append(coco_obj)\n",
    "\n",
    "# random_train_objs = random.sample(train_objs, 5000)\n",
    "\n",
    "# output_file_path = \"/home/nickj/vl-hallucination/data/coco/train_objs_5K.json\"\n",
    "# with open(output_file_path, \"w\") as outfile:\n",
    "#   json.dump(random_train_objs, outfile, indent=4)\n",
    "\n",
    "\n",
    "\n",
    "# Example usage: print the keys of the loaded JSON data\n",
    "# print(coco_data.keys())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = torch.load(\"/home/nickj/vl-hallucination/vl_data/llava7B_train.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "h_ids = []\n",
    "for coco_img in res:\n",
    "  if len(res[coco_img][\"chair_evals\"][\"mscoco_hallucinated_words\"]) > 0:\n",
    "    h_ids.append(coco_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(h_ids, \"/home/nickj/vl-hallucination/vl_data/llava7B_train_ids_with_hallucinations.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "with open(\"/home/nickj/vl-hallucination/data/coco/dataset_coco.json\", \"r\") as f:\n",
    "  coco_data = json.load(f)\n",
    "\n",
    "train_objs = []\n",
    "for coco_obj in coco_data[\"images\"]:\n",
    "  img_path = coco_img_id_to_path(coco_obj[\"imgid\"], validation=False)\n",
    "  if not os.path.exists(img_path):\n",
    "    continue\n",
    "  evals = evaluator.compute_hallucinations(coco_obj[\"imgid\"], \"\")\n",
    "  if len(evals[\"mscoco_gt_words\"]) == 0:\n",
    "    continue\n",
    "\n",
    "  if coco_obj[\"split\"] == \"train\":\n",
    "    train_objs.append(coco_obj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_train_objs = random.sample(train_objs, 5000)\n",
    "\n",
    "output_file_path = \"/home/nickj/vl-hallucination/data/coco/train_objs_5K.json\"\n",
    "with open(output_file_path, \"w\") as outfile:\n",
    "  json.dump(random_train_objs, outfile, indent=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16351"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "directory_path = \"/home/nickj/vl-hallucination/data/coco/train2014/\"\n",
    "num_files = len([name for name in os.listdir(directory_path) if os.path.isfile(os.path.join(directory_path, name))])\n",
    "num_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "with open(\"/home/nickj/vl-hallucination/data/coco/train_objs_5K.json\", \"r\") as f:\n",
    "    coco_data = json.load(f)\n",
    "\n",
    "total = 0\n",
    "issues = 0\n",
    "for obj in coco_data:\n",
    "    img_path = coco_img_id_to_path(obj[\"imgid\"], validation=False)\n",
    "    res = evaluator.compute_hallucinations(obj[\"imgid\"], \"\")\n",
    "    if len(res[\"mscoco_gt_words\"]) == 0:\n",
    "        issues += 1\n",
    "    if not os.path.exists(img_path):\n",
    "        print(f\"Warning: The image path {img_path} does not exist.\")\n",
    "        total += 1\n",
    "print(total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "coco_imgs = []\n",
    "for coco_obj in coco_data:\n",
    "  coco_imgs.append(coco_obj[\"imgid\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(coco_imgs, \"/home/nickj/vl-hallucination/vl_data/train_coco_img_ids.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3947"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "issues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "res = torch.load(\"/home/nickj/vl-hallucination/llava7B_coco.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "176649\n",
      "{'mscoco_hallucinated_words': [('driver', 'person')], 'mscoco_gt_words': [], 'mscoco_generated_words': ['person'], 'hallucination_idxs': [106], 'hallucinated_words': 1, 'recall_words': [], 'recall_idxs': []}\n",
      "564317\n",
      "{'mscoco_hallucinated_words': [('stop sign', 'stop sign'), ('stop sign', 'stop sign')], 'mscoco_gt_words': [], 'mscoco_generated_words': ['stop sign', 'stop sign'], 'hallucination_idxs': [43, 51], 'hallucinated_words': 2, 'recall_words': [], 'recall_idxs': []}\n",
      "449546\n",
      "{'mscoco_hallucinated_words': [('driver', 'person')], 'mscoco_gt_words': [], 'mscoco_generated_words': ['person'], 'hallucination_idxs': [50], 'hallucinated_words': 1, 'recall_words': [], 'recall_idxs': []}\n",
      "309571\n",
      "{'mscoco_hallucinated_words': [], 'mscoco_gt_words': [], 'mscoco_generated_words': [], 'hallucination_idxs': [], 'hallucinated_words': 0, 'recall_words': [], 'recall_idxs': []}\n",
      "220739\n",
      "{'mscoco_hallucinated_words': [], 'mscoco_gt_words': [], 'mscoco_generated_words': [], 'hallucination_idxs': [], 'hallucinated_words': 0, 'recall_words': [], 'recall_idxs': []}\n",
      "383450\n",
      "{'mscoco_hallucinated_words': [('bowl', 'bowl'), ('vase', 'vase'), ('vase', 'vase'), ('vase', 'vase'), ('vase', 'vase'), ('vase', 'vase'), ('vase', 'vase'), ('vase', 'vase'), ('vase', 'vase'), ('bowl', 'bowl'), ('vase', 'vase')], 'mscoco_gt_words': [], 'mscoco_generated_words': ['bowl', 'vase', 'vase', 'vase', 'vase', 'vase', 'vase', 'vase', 'vase', 'bowl', 'vase'], 'hallucination_idxs': [5, 7, 17, 29, 38, 46, 63, 71, 80, 93, 95], 'hallucinated_words': 11, 'recall_words': [], 'recall_idxs': []}\n",
      "310622\n",
      "{'mscoco_hallucinated_words': [('traffic light', 'traffic light'), ('traffic light', 'traffic light')], 'mscoco_gt_words': [], 'mscoco_generated_words': ['traffic light', 'traffic light'], 'hallucination_idxs': [12, 36], 'hallucinated_words': 2, 'recall_words': [], 'recall_idxs': []}\n",
      "92604\n",
      "{'mscoco_hallucinated_words': [], 'mscoco_gt_words': [], 'mscoco_generated_words': [], 'hallucination_idxs': [], 'hallucinated_words': 0, 'recall_words': [], 'recall_idxs': []}\n",
      "228771\n",
      "{'mscoco_hallucinated_words': [], 'mscoco_gt_words': [], 'mscoco_generated_words': [], 'hallucination_idxs': [], 'hallucinated_words': 0, 'recall_words': [], 'recall_idxs': []}\n",
      "513149\n",
      "{'mscoco_hallucinated_words': [('potted plant', 'potted plant')], 'mscoco_gt_words': [], 'mscoco_generated_words': ['potted plant'], 'hallucination_idxs': [100], 'hallucinated_words': 1, 'recall_words': [], 'recall_idxs': []}\n",
      "560371\n",
      "{'mscoco_hallucinated_words': [], 'mscoco_gt_words': [], 'mscoco_generated_words': [], 'hallucination_idxs': [], 'hallucinated_words': 0, 'recall_words': [], 'recall_idxs': []}\n",
      "79331\n",
      "{'mscoco_hallucinated_words': [('driver', 'person')], 'mscoco_gt_words': [], 'mscoco_generated_words': ['person'], 'hallucination_idxs': [37], 'hallucinated_words': 1, 'recall_words': [], 'recall_idxs': []}\n",
      "336873\n",
      "{'mscoco_hallucinated_words': [('driver', 'person'), ('truck', 'truck')], 'mscoco_gt_words': [], 'mscoco_generated_words': ['person', 'truck'], 'hallucination_idxs': [28, 77], 'hallucinated_words': 2, 'recall_words': [], 'recall_idxs': []}\n",
      "568863\n",
      "{'mscoco_hallucinated_words': [('person', 'person')], 'mscoco_gt_words': [], 'mscoco_generated_words': ['person'], 'hallucination_idxs': [56], 'hallucinated_words': 1, 'recall_words': [], 'recall_idxs': []}\n",
      "361831\n",
      "{'mscoco_hallucinated_words': [], 'mscoco_gt_words': [], 'mscoco_generated_words': [], 'hallucination_idxs': [], 'hallucinated_words': 0, 'recall_words': [], 'recall_idxs': []}\n",
      "421673\n",
      "{'mscoco_hallucinated_words': [('driver', 'person')], 'mscoco_gt_words': [], 'mscoco_generated_words': ['person'], 'hallucination_idxs': [82], 'hallucinated_words': 1, 'recall_words': [], 'recall_idxs': []}\n",
      "398454\n",
      "{'mscoco_hallucinated_words': [('driver', 'person'), ('potted plant', 'potted plant')], 'mscoco_gt_words': [], 'mscoco_generated_words': ['person', 'potted plant'], 'hallucination_idxs': [47, 63], 'hallucinated_words': 2, 'recall_words': [], 'recall_idxs': []}\n",
      "198915\n",
      "{'mscoco_hallucinated_words': [('driver', 'person')], 'mscoco_gt_words': [], 'mscoco_generated_words': ['person'], 'hallucination_idxs': [94], 'hallucinated_words': 1, 'recall_words': [], 'recall_idxs': []}\n",
      "493956\n",
      "{'mscoco_hallucinated_words': [], 'mscoco_gt_words': [], 'mscoco_generated_words': [], 'hallucination_idxs': [], 'hallucinated_words': 0, 'recall_words': [], 'recall_idxs': []}\n",
      "402869\n",
      "{'mscoco_hallucinated_words': [('driver', 'person')], 'mscoco_gt_words': [], 'mscoco_generated_words': ['person'], 'hallucination_idxs': [58], 'hallucinated_words': 1, 'recall_words': [], 'recall_idxs': []}\n",
      "92554\n",
      "{'mscoco_hallucinated_words': [], 'mscoco_gt_words': [], 'mscoco_generated_words': [], 'hallucination_idxs': [], 'hallucinated_words': 0, 'recall_words': [], 'recall_idxs': []}\n",
      "29594\n",
      "{'mscoco_hallucinated_words': [('driver', 'person'), ('parking meter', 'parking meter')], 'mscoco_gt_words': [], 'mscoco_generated_words': ['person', 'parking meter'], 'hallucination_idxs': [33, 43], 'hallucinated_words': 2, 'recall_words': [], 'recall_idxs': []}\n",
      "306477\n",
      "{'mscoco_hallucinated_words': [('driver', 'person'), ('truck', 'truck')], 'mscoco_gt_words': [], 'mscoco_generated_words': ['person', 'truck'], 'hallucination_idxs': [34, 65], 'hallucinated_words': 2, 'recall_words': [], 'recall_idxs': []}\n",
      "321603\n",
      "{'mscoco_hallucinated_words': [('bowl', 'bowl'), ('knife', 'knife'), ('spoon', 'spoon')], 'mscoco_gt_words': [], 'mscoco_generated_words': ['bowl', 'knife', 'spoon'], 'hallucination_idxs': [61, 72, 85], 'hallucinated_words': 3, 'recall_words': [], 'recall_idxs': []}\n",
      "397287\n",
      "{'mscoco_hallucinated_words': [], 'mscoco_gt_words': [], 'mscoco_generated_words': [], 'hallucination_idxs': [], 'hallucinated_words': 0, 'recall_words': [], 'recall_idxs': []}\n",
      "476491\n",
      "{'mscoco_hallucinated_words': [('fire hydrant', 'fire hydrant'), ('parking meter', 'parking meter')], 'mscoco_gt_words': [], 'mscoco_generated_words': ['fire hydrant', 'parking meter'], 'hallucination_idxs': [61, 74], 'hallucinated_words': 2, 'recall_words': [], 'recall_idxs': []}\n",
      "10440\n",
      "{'mscoco_hallucinated_words': [], 'mscoco_gt_words': [], 'mscoco_generated_words': [], 'hallucination_idxs': [], 'hallucinated_words': 0, 'recall_words': [], 'recall_idxs': []}\n",
      "317120\n",
      "{'mscoco_hallucinated_words': [], 'mscoco_gt_words': [], 'mscoco_generated_words': [], 'hallucination_idxs': [], 'hallucinated_words': 0, 'recall_words': [], 'recall_idxs': []}\n",
      "451373\n",
      "{'mscoco_hallucinated_words': [('car', 'car')], 'mscoco_gt_words': [], 'mscoco_generated_words': ['car'], 'hallucination_idxs': [89], 'hallucinated_words': 1, 'recall_words': [], 'recall_idxs': []}\n",
      "9759\n",
      "{'mscoco_hallucinated_words': [], 'mscoco_gt_words': [], 'mscoco_generated_words': [], 'hallucination_idxs': [], 'hallucinated_words': 0, 'recall_words': [], 'recall_idxs': []}\n",
      "277329\n",
      "{'mscoco_hallucinated_words': [('person', 'person'), ('car', 'car'), ('villager', 'person')], 'mscoco_gt_words': [], 'mscoco_generated_words': ['person', 'car', 'person'], 'hallucination_idxs': [46, 73, 83], 'hallucinated_words': 3, 'recall_words': [], 'recall_idxs': []}\n",
      "466958\n",
      "{'mscoco_hallucinated_words': [], 'mscoco_gt_words': [], 'mscoco_generated_words': [], 'hallucination_idxs': [], 'hallucinated_words': 0, 'recall_words': [], 'recall_idxs': []}\n"
     ]
    }
   ],
   "source": [
    "total = 0\n",
    "for coco_img in res:\n",
    "  # evals = evaluator.compute_hallucinations(coco_img, res[coco_img][\"caption\"])\n",
    "  img_path = coco_img_id_to_path(coco_img, validation=True)\n",
    "  if len(evals[\"mscoco_gt_words\"]) == 0:\n",
    "    print(coco_img)\n",
    "    print(evals)\n",
    "    total += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_imgs = torch.load(\"/home/nickj/vl-hallucination/vl_data/train_coco_img_ids.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from experiments.embeddings import coco_img_id_to_path\n",
    "import shutil\n",
    "for obj in train_imgs:\n",
    "    file_name = f\"COCO_train2014_{(12 - len(str(obj))) * '0' + str(obj)}\"\n",
    "    img_path = f\"/home/nickj/vl-hallucination/data/coco/COCO_2014_YOLOv3_raw/images/train2014/{file_name}.jpg\"\n",
    "    # res = evaluator.compute_hallucinations(obj[\"imgid\"], \"\")\n",
    "    if os.path.exists(img_path):\n",
    "        destination_path = os.path.join(\"/home/nickj/vl-hallucination/vl_data/train2014\", os.path.basename(img_path))\n",
    "        os.makedirs(os.path.dirname(destination_path), exist_ok=True)\n",
    "\n",
    "        shutil.move(img_path, destination_path)\n",
    "    else:\n",
    "        print(f\"Warning: The image path {img_path} does not exist.\")\n",
    "        total += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting annotations for 0/896782 segmentation masks"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'set' object has no attribute 'append'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mevaluator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_annotations\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/vl-hallucination/metric/chair.py:285\u001b[0m, in \u001b[0;36mCHAIR.get_annotations\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    279\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_annotations\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    281\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m'''\u001b[39;00m\n\u001b[1;32m    282\u001b[0m \u001b[38;5;124;03m    Get annotations from both segmentation and captions.  Need both annotation types for CHAIR metric.\u001b[39;00m\n\u001b[1;32m    283\u001b[0m \u001b[38;5;124;03m    '''\u001b[39;00m\n\u001b[0;32m--> 285\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_annotations_from_segments\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m \n\u001b[1;32m    286\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_annotations_from_captions()\n\u001b[1;32m    287\u001b[0m     \u001b[38;5;66;03m# deduplicate\u001b[39;00m\n",
      "File \u001b[0;32m~/vl-hallucination/metric/chair.py:257\u001b[0m, in \u001b[0;36mCHAIR.get_annotations_from_segments\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    254\u001b[0m     imid \u001b[38;5;241m=\u001b[39m annotation[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mimage_id\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m    256\u001b[0m     node_word \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minverse_synonym_dict[id_to_name[annotation[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcategory_id\u001b[39m\u001b[38;5;124m'\u001b[39m]]]\n\u001b[0;32m--> 257\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimid_to_objects\u001b[49m\u001b[43m[\u001b[49m\u001b[43mimid\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mappend\u001b[49m(node_word)\n\u001b[1;32m    258\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'set' object has no attribute 'append'"
     ]
    }
   ],
   "source": [
    "evaluator.get_annotations()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
