{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from methods.llava_utils import load_llava_state\n",
    "from methods.blip_utils import get_phrase_embedding, load_blip_state\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import pickle\n",
    "from methods.algorithms import generate_mass_edit_pre_hook\n",
    "import torch\n",
    "import random\n",
    "\n",
    "torch.set_grad_enabled(False)\n",
    "\n",
    "os.chdir(os.environ[\"VL_ROOT_DIR\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_type = \"llava7b\"\n",
    "if model_type == \"llava7b\":\n",
    "    # Load the LlaVA model\n",
    "    loaded_state = load_llava_state(model_type, train = True)\n",
    "  elif model_type.startswith(\"blip\"):\n",
    "    loaded_state = load_blip_state(model_type, train = True)\n",
    "  else:\n",
    "    raise Exception(f\"model type {model_type} not supported\")\n",
    "\n",
    "  vocabulary, vocab_embeddings, data, execute_model, register_pre_hook, tokenizer = loaded_state[\"vocabulary\"], loaded_state[\"vocab_embeddings\"], loaded_state[\"data\"], loaded_state[\"execute_model\"], loaded_state[\"register_pre_hook\"], loaded_state[\"tokenizer\"]\n",
    "\n",
    "  id_to_token = dict()\n",
    "  for word in vocabulary:\n",
    "    id_to_token[vocabulary[word]] = word\n",
    "\n",
    "  output_file_path = f\"./vl_results/{output_file}\"\n",
    "  if os.path.exists(output_file_path):\n",
    "    results = torch.load(output_file_path)\n",
    "  else:\n",
    "    results = dict()\n",
    "\n",
    "  random.seed(1)\n",
    "  sampled_coco_img_ids = random.sample(list(data.keys()), sample_size)\n",
    "\n",
    "  img_count = 0\n",
    "  cache = dict()\n",
    "  for coco_img in tqdm(sampled_coco_img_ids, desc=f\"Experiment 10A: mass editing\"):\n",
    "    img_count += 1\n",
    "    if img_count % 100 == 0:\n",
    "      torch.save(results, f\"./vl_results/{output_file}\")\n",
    "\n",
    "    if coco_img in results:\n",
    "      continue\n",
    "\n",
    "    # Collect the text embeddings that correspond with the hallucinations\n",
    "    text_embeddings = []\n",
    "    if remove_hallucinations:\n",
    "      for caption_word, coco_class in set(data[coco_img][\"chair_evals\"][\"mscoco_hallucinated_words\"]):\n",
    "        text_embeddings.append(get_phrase_embedding(caption_word, vocab_embeddings, tokenizer))\n",
    "\n",
    "    if remove_gt:\n",
    "      for caption_word, coco_class in set(data[coco_img][\"chair_evals\"][\"recall_words\"]):\n",
    "        text_embeddings.append(get_phrase_embedding(caption_word, vocab_embeddings, tokenizer))\n",
    "\n",
    "    if len(text_embeddings) == 0:\n",
    "      print(\"Continuing\")\n",
    "      continue\n",
    "\n",
    "    text_embeddings = torch.stack(text_embeddings, dim = 0)\n",
    "\n",
    "    # Create a hook that will make the residual embedding orthogonal to these text embeddings\n",
    "    if model_type.startswith(\"llava\"):\n",
    "      edit_embeddings_hook = generate_mass_edit_pre_hook(text_embeddings, start_edit_index=35, end_edit_index=-12, layer=0, weight = weight_factor, minimum_size=576)\n",
    "    else:\n",
    "      edit_embeddings_hook = generate_mass_edit_pre_hook(text_embeddings, start_edit_index=0, end_edit_index=32, layer=0, weight = weight_factor, minimum_size=32)\n",
    "    hook = register_pre_hook(edit_embeddings_hook, layer = 0)\n",
    "\n",
    "    # Rerun the model with the hooks enabled\n",
    "    new_caption = execute_model(coco_img)\n",
    "\n",
    "    # Compute the hallucinations\n",
    "    new_chair_eval = evaluator.compute_hallucinations(coco_img, new_caption)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
